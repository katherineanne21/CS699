---
title: "CS 699 Assignment 4"
author: "Katherine Rein"
output:
  pdf_document:
    latex_engine: xelatex
    pandoc_args: "--variable=geometry:margin=1in"
---

```{r}
# Import libraries
library(MASS)
library(rsample)
library(caret)
library(tidyverse)
library(nnet)
library(xgboost)
```


# Problem 1

(1). Generate training and holdout partitions on the data set. Use 1/3 of the data in the
holdout.
(2). Fit a discriminant analysis model. Use the discriminant analysis model to make
predictions on the holdout data. Generate a confusion matrix and measure the
model’s performance using one or more appropriate metrics of your choice.
(3). Fit a neural network model, applying a training grid to tune the parameters. Use the
neural network model to make predictions on the holdout data. Generate a
confusion matrix and measure the model’s performance using one or more
appropriate metrics of your choice.
(4). Fit a random forest model, applying a training grid to tune the parameters. Use the
random forest model to make predictions on the holdout data. Generate a confusion
matrix and measure the model’s performance using one or more appropriate metrics
of your choice.
(5). Fit a support vector machine, applying a training grid to tune the parameters. Use
the support vector machine to make predictions on the holdout data. Generate a
confusion matrix and measure the model’s performance using one or more
appropriate metrics of your choice.
(6). Compare the four models. Which, if any, seems to perform better? Discuss.
```{r} 
# Read in data
acc_data = read.csv('accidents1000.csv')

# Ensure the class column is categorical
acc_data$MAX_SEV <- factor(acc_data$MAX_SEV)

# Make data usable
acc_data_scaled <- acc_data %>% mutate(across(SPD_LIM, ~ ( . - min(.)) / (max(.) - min(.))))
acc_data_scaled$MAX_SEV <- factor(make.names(acc_data_scaled$MAX_SEV))

# Split train and test
set.seed(42)
split <- initial_split(acc_data_scaled, prop = 2/3, strata = MAX_SEV) 
train <- training(split)
test <- testing(split)

# Fit a DA model
da_model <- lda(MAX_SEV ~ ., data = train)
pred <- predict(da_model, test)

performance_measures <- confusionMatrix(pred$class, test$MAX_SEV)
print('DA MODEL')
print(performance_measures)

# Fit a neural network
ctrl <- trainControl(method = "CV", number = 10,
                     summaryFunction = defaultSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

nnetGrid <- expand.grid(size = 1:13, decay = seq(0, 2, 0.2))

nnetFit <- train(x = train[, colnames(train) != "MAX_SEV"], 
                 y = train$MAX_SEV,
                 method = "nnet",
                 preProc = c("center", "scale"),
                 tuneGrid = nnetGrid,
                 trace = FALSE,
                 maxit = 100,
                 MaxNWts = 1000,
                 trControl = ctrl)

test_pred <- predict(nnetFit, newdata = test)
performance_measures <- confusionMatrix(test_pred, test$MAX_SEV)
print('NEURAL NETWORK')
print(performance_measures)

# Fit a random forest
ctrl <- trainControl(method = "CV",
                     summaryFunction = defaultSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

mtryValues <- seq(2, ncol(train) - 1, by = 1)
rfFit <- caret::train(x = train[, colnames(train) != "MAX_SEV"], 
                      y = train$MAX_SEV,
                      method = "rf",
                      ntree = 100,
                      tuneGrid = data.frame(mtry = mtryValues),
                      importance = TRUE,
                      metric = "Accuracy",
                      trControl = ctrl)

pred <- predict(rfFit, newdata = test)
performance_measures <- confusionMatrix(pred, test$MAX_SEV)
print('RANDOM FOREST')
print(performance_measures)

# Fit a SVM
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 5, 
                              summaryFunction = defaultSummary)
svmGrid <-  expand.grid(sigma = seq(0.06, 0.3, by = 0.06), C = seq(0.5, 1.5, by = 0.1))
svm.model <- train(MAX_SEV ~ ., data = train, method = "svmRadial",
                   preProc = c("center", "scale"),
                   trControl = train_control, tuneGrid = svmGrid)

pred <- predict(svm.model, test)
performance_measures <- confusionMatrix(pred, test$MAX_SEV)
print('SVM')
print(performance_measures)

```

```
Discriminant Analysis Model: This model has an accuraccy of 0.4551 which feels
very low. I would argue that with such a low accuracy this model is not a good
model.

Nueral Network: This model has an accuraccy of 0.4671 which is better but still
not great. Overall this is a better model but I would not say it is good.

Random Forest: This model has an accuraccy of 0.485 which is also better but none
of these models so far are making massive jumps in accuracy.

SVM: This model is slightly worse than random forest with an accuracy of 0.479.

Overall, Random Forest has the best accuracy so I would most likely choose it for
my model of choice.

```


# Problem 2

(1). Generate training and holdout partitions on the data set. Use 1/3 of the data in the
holdout.
(2). Fit an XGBoost tree model, applying a training grid to tune the parameters. Use the
XGBoost tree model to make predictions on the holdout data. Measure the tree’s
performance using one or more appropriate metrics of your choice.
(3). Fit a random forest model, applying a training grid to tune the parameters. Use the
random forest model to make predictions on the holdout data. Measure the forest’s
performance using one or more appropriate metrics of your choice.
(4.) Compare the XGBoost tree model to the random forest model. Which, if any, seems
to perform better? Discuss.
```{r} 
# Load data (predictive column = Revenue)
rest_data = read.csv('restaurantdata.csv')

# Train test split
set.seed(42)
split <- initial_split(rest_data, prop = 2/3, strata = Revenue) 
train <- training(split)
test <- testing(split)

# Fit an XGBoost tree
xgb_control = trainControl(
  method = "cv", number = 10,
  summaryFunction = defaultSummary
)

xgbGrid <- expand.grid(
  nrounds = c(100, 200),
  eta = c(0.1),
  max_depth = c(2),
  gamma = c(0),
  colsample_bytree = 1,
  min_child_weight = c(1),
  subsample = c(0.7)
)

xgbModel <- train(
  Revenue ~ .,
  data = train,
  method = "xgbTree",
  trControl = xgb_control,
  tuneGrid = xgbGrid,
  metric = "RMSE",
  verbose = FALSE,
  verbosity = 0
)


pred <- predict(xgbModel, test)
performance_measures <- postResample(pred, test$Revenue)
print('XGBOOST TREE')
print(performance_measures)

# Random Forest
ctrl <- trainControl(method = "CV",
                     summaryFunction = defaultSummary,
                     number = 5)

mtryValues <- c(3, 6, 9, 12)
#mtryValues <- seq(2, ncol(train) - 1, by = 1)
rfFit <- train(Revenue ~ ., data = train, 
               method = "rf", 
               ntree = 100,
               tuneGrid = data.frame(mtry = mtryValues),
               importance = TRUE,
               metric = "RMSE",
               trControl = ctrl)

pred <- predict(rfFit, newdata = test)
performance_measures <- postResample(pred, test$Revenue)
print('RANDOM FOREST')
print(performance_measures)

```

```
XGBoost Tree: The R squared is 0.9984 which is really good. This means that the model
accounts for 99.8% of the variability in the data. This is a really good model.

Random Forest: The R squared is 0.9987 which is also really good.

From just looking at the R sqaured values, the Random Forest model seems to be better. This
is because it has a higher R squared value. The higher the R squared value the
more the model is predicting the varibility of the data. However, because they are
so similar and XGBoost Tree took significantly less time to run I would probably
still choose XGBoost Tree
```

